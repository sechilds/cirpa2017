---
title: "CIRPA Workshop"
author: "Stpehen Childs"
date: '2017-10-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(haven)
library(labelled)
library(readxl)
library(modelr)
```

# CIRPA 2017 Workshop

I have discovered something - you can use macros in `vim` mode in RStudio.
That is pretty cool.

## Outline

* Loading data (review)
* Loading survey data
* Manipulating survey data - for easier vizualization and analysis.
* descriptive statistics (means, medians, etc.)
* exploratory data visualization (with `ggplot2`)
* the R formula language
* statistical tests
* the `broom` package - cleaning up results
* statistical modeling
* categorical data
* predictive models/machine learning
* ethics of algorithms
* descriptive models/econometrics
* outputting results

I want to think about some concept maps for this stuff.
There are quite a few topics here, and I want to catch them all.

### Loading Data

This might be a review of stuff that Evan has talked about.

* reading data from CSV files with `read_csv`
* reading data from Excel files (ASK EVAN) - try the `readxl` package
* reading data from SPSS files with `haven`

```{r}
rdm_survey <- read_xlsx('data/uOttawa_RDM_Survey_Data_Anonymized.xlsx')
```


### Loading Survey Data

Survey data can be a bit of a special case because of the associated metadata around the survey.
You want to have that metadata, but it can be difficult to process.

I will talk about `haven` and how you can use it to load up SPSS data -- and how you can
grab things out of the resulting data frame with `labelled`.

```{r}
# NSSE Sample Data
nsse <- read_sav('data/NSSE17 Data (Canadian Institution).sav')
nsse
```

This sample NSSE data is in the same format as you would download your own data.
You will want to extract the questions from there -- since they are included as part of the SPSS file.

```{r}
# you can get a data frame pretty easily -- use as.data.frame and then gather it.
# you will get a warning that you are throwing away data
#as.data.frame(var_label(nsse)) %>% gather()
nsse %>%
  var_label() %>%
  as_tibble() %>%
  gather(field_name, question_text)
```

```{r}
nsse$askquest
```

```{r}
mesa <- read_csv('data/cmsf-mesa-E-2005-08_F1.csv')
```

### Manipulating Survey Data for Ease of Analysis

I want to talk about how you might use `dplyr` to layout the data in a more efficient way.
the `tidyr` functions `gather` and `spread` are very useful for this.
The challenge of this is that it makes the data quite large.
And I only really want a simple version of this.

I'm not sure I can talk about weights.

I have that short sample NSSE file, so I will use that to talk about survey data manipulation.
Becuase that has nice labeled values -- and you want to grab all of that data.

Survey data can be a pain to work with because it contains a lot of meta data:

* full question text
* other text that provides context for a question
* question number (order of questions in a survey)
* question section and subsection
* variable labels (what the numeric responses actually mean.)

The problem is that you need some of these items to do your analysis
and other items to do the presentation of that analysis.

I propose constructing a large dataset consisting of several talbes made to be joined together.
If you have access to a database, you might want to use that.

### Descriptive Statistics

When you have basic data loaded, you want to get all sorts of descriptive statistics out of there.
So, there are lots of built in functions that will do that for you.

I want to look at the difference between the base and dplyr functions.
Actually, the `mean` function you use with `summarise` is the same as the base function.
That is actually intersting.

One issue with survey data, and the likert scales that we generally use -- is that we tend to think of these things as numbers when they aren't really.

## Exploratory Data Viz

You want to start out with some simple data vizualization to get a sense of what things look like. We can plot histograms or do kernel density plots.

```{r}
df %>%
  ggplot() + 
  geom_histogram(aes(x = t1_y1))
```

## the R formula language

The R formula language is a standard part of R, and it is used to specify equations and even subsetting graphs and statistical tests.

It lets you write out model equations the way you would write them mathematically.
You can use the `model_matrix()` function to see what they are doing.

The `model_matrix()` function is a part of the `modelr` package and is a wrapper around
`model.matrix()`. Basically, so it works better with the tidyverse.

```{r}
model_matrix(mesa, )

```


## statistical tests

Since R was designed by statisticians, it has all of these tests built into the base R language.

A statistical test is getting at the idea that we want to see how likely that a particular result that we see could happen at random.
For example, if we are comparing two means (say NSSE scores between two universities),
we can look at which number is bigger and say that institution did better.
But, we also want to see how likely it was that one was higher than the other randomly.

Comparing something from a survey isn't like comparing, say, total grant funding.
In the survey we are working with an estimate based on a sample of the population.
With accounting numbers, we can just do the comparison.

Statistical tests are based on probability distributions and they let us tell if the differences we see are more likely to be related to the sample than the actual differences.

R was designed by statisticians, and so has an advantage in that all the probability distributions are built into the language. 

There is a list at this site: [http://www.stat.umn.edu/geyer/old/5101/rlook.html](http://www.stat.umn.edu/geyer/old/5101/rlook.html)

Each distribution has four functions that you can use for distribution.

## Normal Distribution
```{r}
# normal distribution - density plot
ggplot(data.frame(x = c(-2,2)), aes(x)) + stat_function(fun = dnorm)
```

```{r}
# normal distribution - cdf plot
ggplot(data.frame(x = c(-2,2)), aes(x)) + stat_function(fun = pnorm)
```

```{r}
# normal distribution - inverse cdf plot
ggplot(data.frame(x = c(-0,1)), aes(x)) + stat_function(fun = qnorm)
```

```{r}
# normal distribution - random values
x = rnorm(1000)
#hist(x)
#qplot(x, geom="histogram")
ggplot(data = tibble(x = x), aes(x = x)) + geom_histogram()
```

```{r}
# you can also get the density values and plot them
d <- density(x)
plot(d)
```

```{r}
# same plot using ggplot2
ggplot(data = tibble(x = x), aes(x = x)) + geom_density()
```
  

## T-Distribution
```{r}
# t distribution
ggplot(data.frame(x = c(-2,2)), aes(x)) + stat_function(fun = dt, args = list(df = 4000))
```

```{r}
ggplot(data.frame(x = c(-2,2)), aes(x)) + stat_function(fun = dt, args = list(df = 30)) +   stat_function(fun = dnorm, color = 'red')
```

```{r}
# Chi Squared df=7 and .95 area highlighted.
ggplot(data.frame(x = c(0,20)), aes(x)) +
  stat_function(fun = dchisq, args = list(df=7)) +
  stat_function(fun = dchisq, args = list(df=7),
                xlim = c(qchisq(.95, df=7),20),
                geom = 'area')
```

```{r}
# T-Distribution df=7 and .95 area highlighted.
ggplot(data.frame(x = c(-5,5)), aes(x)) +
  stat_function(fun = dt, args = list(df=5)) +
  stat_function(fun = dt, args = list(df=5),
                xlim = c(qt(.95, df=5),5),
                geom = 'area') +
  stat_function(fun = dt, args = list(df=5),
                xlim = c(-5, -qt(.95, df=5)),
                geom = 'area')
```
## Cleaning Up Results

In this section, we will discuss deploying the `broom` package to clean up the results of statistical tests. This is especially useful for running large numbers of statistical tests and putting the results into a DataFrame.

Test the difference between two groups.
Determine if something is significantly different from zero.
We will just explain the high level of this, I think.
A few examples will be instructive.

## statistical modeling

Now that we have dome some basic statistical tests, the next step is to build models.
Statistical models are, at their core, just an equation that we are trying to fit to the data.
We try to describe the relationships between different variables using match - the functional form.
Then we figure out what the parameters of those models are.

## categorical data

The standard OLS (Ordinary Least Squares) model is set up to model a linear relationship between variables.
We take that linear relationship, and the slope of the line gives us the realtionship.
We can standardize the variables, so that slope represents the rate of change in one variable with respect to another.
(This calculus insight is useful for the marginal effects bit as well.
You want to put these things in terms of percentage change.)

The difficulty of categorical data is that the categories are _discrete_,
so there isn't really a rate of change, but rather a jump as you move between two categories.

If you think of the simple linear model: \( y = \alpha + \beta x \)
the slope is what you are generally measuring. (\(\beta\))
But, if you have two groups in the model - say students at two different campuses:

\(
  \begin{align}
  y = \alpha + \beta _1 x & \textnormal{if} c = 1) \\
  y = \alpha + \beta _1 x + \beta _2 & \textnormal{if} c = 2)
  \end{align}
\)
y = a + B1x if campus 1
  = a + B1x + B2 if campus 2
  
That means the intercept for campus2 is (a + B2) - so the linear model shifts up or down the graph.
You can think of the difference in the intercepts (B2) as capturing the difference in outcome between the two groups.

Of course, it's not quite that simple.
We are making some assumptions when we set up a model like that.
If we leave out important variables that are different between the two groups,
we might actually be capturing the difference between other things.
For example, if the students one one campus are older than the other,
we might be capturing the effects of that.

Another reason why it is important to graph this data before setting up the model.
And why you can get results out quickly with R,
but you should take your time and know aobut your data before making your analysis.

## Predictive Models/Machine Learning

R is widely used in the machine learning community.
And support for these kind of models is built right in.

Predictive models take the equations we talked about earlier and let you plug new values into them.
Then you can use the outcomes that you calculate from that model to make predictions aobut individual observations that you have data on.

In the press this is sometimes called Artificial Intelligence (AI),
but practitioners generally refer to it as Machine Learning.
That name sounds impressive, but we are really teaching the computer with models.

Now, there are fancy techniques in machine learning (neural networks, etc.) that get used for lots of really cool things (image recognition, speach recognition, etc.)
But at their heart, they are basically doing the same thing as we are with the models we use here.
Some of these models are just taking advantage of a lot more data.

One of the key things to understand this domain is that just about anything can be turned into a number!
Images, sound or even text.

Talking about training and test data -- how you fit models and then evaluate them.

## Ethics of Algorithms

As our world beings to be ruled more and more by algorithms,
we need to take seriously how we implement them.
We want to use them to support our students and help them succeed,
not unfairly discriminate against them.

## Descriptive Models/Econometrics

The other way to use models is to try to draw general observations about the "world" from them.
In this use of models, we aren't as interested in individual predictions, but rather at higher levels.

This lets us ask questions about things relate to each other.
Depending on our audience, these can be very general things or specific to our institution.

The problematique and then turning that into a research question.

This fits into IR in several ways.
First of all, we are interested in overall relationships between (say) student characteristics at the high level.
So it's worth it to understand that there is a relationship between high school grades and student retention.
But we might also want to use that information to look at outcomes across our institutions.
Say, if we observe different retention rates across facuilties,
does the same issue hold when we take high school grades into account.
(e.g. is retention in faculty A better than B because A is highly competitive to get into and we are only admitting students with higher grades?)

This is where the theory comes in,
you need to understand what you are doing before you just apply these models.

## outputting results

Broom will tidy up things for you.
There are also some R packages that will output reslts nicely.

You can use `broom` to tidy up the coefficients and then print out the models using `kable`.

There are other regression table formats that spit out LaTeX, which is probably beyond the scope of the workshop.











This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
